{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rachanabn20/MUCS-RANLP2025/blob/main/PolyHope_M_at_RANLP2025ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YDr4heBWbp_"
      },
      "source": [
        "# ***PolyHope-M at RANLP2025***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4zg00nCmWWN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d633afd7-eeb8-443d-cef0-1649f8126a6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QqOSOK31WWQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "057b2643-eee0-4e90-a0b5-fe958e1ce2ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting num2words\n",
            "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting docopt>=0.6.2 (from num2words)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=2022f6932ea39fb2c92d54b9b9f7ca9bef4427353393fd64e94d0dfc020e4ab5\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.14\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n",
            "Collecting emot\n",
            "  Downloading emot-3.1-py3-none-any.whl.metadata (396 bytes)\n",
            "Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emot\n",
            "Successfully installed emot-3.1\n",
            "Collecting demoji\n",
            "  Downloading demoji-1.1.0-py3-none-any.whl.metadata (9.2 kB)\n",
            "Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: demoji\n",
            "Successfully installed demoji-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install num2words\n",
        "!pip install emoji\n",
        "!pip install contractions\n",
        "!pip install emot\n",
        "!pip install demoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iY-bDJl3WWS6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import contractions\n",
        "import re\n",
        "import regex\n",
        "import demoji\n",
        "import emoji\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import f1_score\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from num2words import num2words\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Az-DGNhVWWVo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d059db-f3e4-472b-cb66-98260d257b56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Download necessary NLTK data files\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "#nltk.download('whitespace')\n",
        "\n",
        "# Define stopwords for each language\n",
        "stopwords_en = set(stopwords.words('english'))\n",
        "stopwords_de = set(stopwords.words('german'))\n",
        "stopwords_es = set(stopwords.words('spanish'))\n",
        "\n",
        "# Manually define Urdu stop words\n",
        "stopwords_ur = set([\n",
        "    \"اور\", \"کا\", \"کی\", \"کے\", \"سے\", \"میں\", \"تو\", \"پر\", \"نے\", \"یہ\", \"ہے\", \"ہی\", \"کو\", \"نہیں\",\n",
        "    \"اس\", \"کر\", \"لیکن\", \"کرنا\", \"کرنے\", \"کرتے\", \"کیا\", \"ہو\", \"جا\", \"سکتے\", \"وہ\", \"ہوتا\",\n",
        "    \"ہوتی\", \"ہوتے\", \"تھا\", \"ہوئے\", \"بھی\", \"کسی\", \"کبھی\", \"کہاں\", \"کیوں\", \"کچھ\",\n",
        "    \"لیا\", \"دیا\", \"دیے\", \"دیکھا\", \"گیا\", \"جب\", \"تب\", \"اگر\", \"مگر\", \"یہاں\", \"وہاں\", \"کہ\",\n",
        "    \"کہا\", \"ساتھ\", \"بار\", \"دوسرے\", \"پہلے\", \"بعد\", \"چاہیے\", \"جیسے\", \"ایسے\", \"تیسرے\",\n",
        "    \"چوتھے\", \"پانچویں\", \"چھٹے\", \"ساتویں\", \"آٹھویں\", \"نویں\", \"دسویں\", \"گیارہویں\", \"بارہویں\"\n",
        "])\n",
        "\n",
        "# Combine all stopwords for the remove_stopwords function\n",
        "all_stopwords = stopwords_en.union(stopwords_de, stopwords_es, stopwords_ur)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cLcDpCpGWWYS"
      },
      "outputs": [],
      "source": [
        "# English train and dev datasets\n",
        "en_train = pd.read_csv('/content/drive/MyDrive/Training_Data_PolyHope-M_2025/Training Data/en_train.csv')\n",
        "en_dev = pd.read_csv('/content/drive/MyDrive/Training_Data_PolyHope-M_2025/Training Data/en_dev.csv')\n",
        "\n",
        "# Spanish train and dev datasets\n",
        "es_train = pd.read_csv('/content/drive/MyDrive/Training_Data_PolyHope-M_2025/Training Data/es_train.csv')\n",
        "es_dev= pd.read_csv('/content/drive/MyDrive/Training_Data_PolyHope-M_2025/Training Data/es_dev.csv')\n",
        "\n",
        "#Urdu train and dev datasets\n",
        "ur_train = pd.read_csv('/content/drive/MyDrive/Training_Data_PolyHope-M_2025/Training Data/Ur_train.csv')\n",
        "ur_dev= pd.read_csv('/content/drive/MyDrive/Training_Data_PolyHope-M_2025/Training Data/Ur_dev.csv')\n",
        "\n",
        "#German train and dev datasets\n",
        "de_train = pd.read_csv('/content/drive/MyDrive/Training_Data_PolyHope-M_2025/Training Data/German_train.csv')\n",
        "de_dev= pd.read_csv('/content/drive/MyDrive/Training_Data_PolyHope-M_2025/Training Data/German_dev.csv')\n",
        "\n",
        "# Test sets\n",
        "en_test=pd.read_csv('/content/drive/MyDrive/TestWithNoLabel/TestWithNoLabel/en_test_without_labels.csv')\n",
        "es_test=pd.read_csv('/content/drive/MyDrive/TestWithNoLabel/TestWithNoLabel/es_test_without_labels.csv')\n",
        "ur_test=pd.read_csv('/content/drive/MyDrive/TestWithNoLabel/TestWithNoLabel/Ur_test_without_labels.csv')\n",
        "de_test=pd.read_csv('/content/drive/MyDrive/TestWithNoLabel/TestWithNoLabel/German_test_without_labels.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ddgM8Dh0WWau"
      },
      "outputs": [],
      "source": [
        "merge_train = pd.concat([en_train, es_train,ur_train,de_train], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oapM_SSYb4XI"
      },
      "outputs": [],
      "source": [
        "# Text preprocessing\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def remove_stopwords(text, language='english'):\n",
        "    stopwords_set = {\n",
        "        'english': stopwords_en,\n",
        "        'german': stopwords_de,\n",
        "        'spanish': stopwords_es,\n",
        "        'urdu': stopwords_ur\n",
        "    }\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stopwords_set[language]]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "def lemmatize_text(text, language='english'):\n",
        "    if language == 'english':\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        words = word_tokenize(text)\n",
        "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "        return ' '.join(lemmatized_words)\n",
        "    else:\n",
        "        # For other languages, return the text as is\n",
        "        return text\n",
        "\n",
        "def stem_text(text, language='english'):\n",
        "    stemmer = SnowballStemmer(language)\n",
        "    words = word_tokenize(text)\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "def convert_numbers_to_words(text, language='en'):\n",
        "    superscript_map = str.maketrans('⁰¹²³⁴⁵⁶⁷⁸⁹', '0123456789')\n",
        "    text = text.translate(superscript_map)\n",
        "    tokens = word_tokenize(text)\n",
        "    converted_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.isdigit():\n",
        "            try:\n",
        "                converted_tokens.append(num2words(int(token), lang=language))\n",
        "            except NotImplementedError:\n",
        "                converted_tokens.append(token)\n",
        "        else:\n",
        "            converted_tokens.append(token)\n",
        "    return ' '.join(converted_tokens)\n",
        "\n",
        "def remove_non_ascii(text):\n",
        "    return ''.join(char for char in text if ord(char) < 128)\n",
        "\n",
        "def remove_tags(text):\n",
        "    return re.sub(r'#\\w+', '', text)\n",
        "\n",
        "def preprocess_text(text, language='english'):\n",
        "    text = text.lower()\n",
        "    text = remove_non_ascii(text)\n",
        "    text = expand_contractions(text)\n",
        "    text = remove_tags(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = convert_numbers_to_words(text, language)\n",
        "    text = remove_stopwords(text, language)\n",
        "    text = lemmatize_text(text, language)\n",
        "    text = stem_text(text, language)\n",
        "    return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "i7E7UTuKWWfz"
      },
      "outputs": [],
      "source": [
        "# Preprocess the text data\n",
        "merge_train['processed_text'] = merge_train['text'].apply(lambda x: preprocess_text(x))\n",
        "en_dev['processed_text'] = en_dev['text'].apply(lambda x: preprocess_text(x))\n",
        "es_dev['processed_text'] = es_dev['text'].apply(lambda x: preprocess_text(x))\n",
        "ur_dev['processed_text'] = ur_dev['text'].apply(lambda x: preprocess_text(x))\n",
        "de_dev['processed_text'] = de_dev['text'].apply(lambda x: preprocess_text(x))\n",
        "en_test['processed_text'] = en_test['text'].apply(lambda x: preprocess_text(x))\n",
        "es_test['processed_text'] = es_test['text'].apply(lambda x: preprocess_text(x))\n",
        "ur_test['processed_text'] = ur_test['text'].apply(lambda x: preprocess_text(x))\n",
        "de_test['processed_text'] = de_test['text'].apply(lambda x: preprocess_text(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Binary Classification**"
      ],
      "metadata": {
        "id": "iC5k8F60a0E_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dev Set**"
      ],
      "metadata": {
        "id": "FKDfTG7ha5Ho"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6uT9_g3YWWie"
      },
      "outputs": [],
      "source": [
        "\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
        "    ('clf', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "\n",
        "pipeline.fit(merge_train['processed_text'], merge_train['binary'])\n",
        "\n",
        "\n",
        "y_pred_es = pipeline.predict(es_dev['processed_text'])\n",
        "y_pred_en = pipeline.predict(en_dev['processed_text'])\n",
        "y_pred_ur = pipeline.predict(ur_dev['processed_text'])\n",
        "y_pred_de = pipeline.predict(de_dev['processed_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Classification Report for Spanish (es):\")\n",
        "print(classification_report(es_dev['binary'], y_pred_es))\n",
        "\n",
        "print(\"Classification Report for English (en):\")\n",
        "print(classification_report(en_dev['binary'], y_pred_en))\n",
        "\n",
        "print(\"Classification Report for Urdu (ur):\")\n",
        "print(classification_report(ur_dev['binary'], y_pred_ur))\n",
        "\n",
        "print(\"Classification Report for German (de):\")\n",
        "print(classification_report(de_dev['binary'], y_pred_de))"
      ],
      "metadata": {
        "id": "LaGC1DGNxBGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e3edb4-9fb1-45bc-963a-dc8d158bce80"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Spanish (es):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hope       0.77      0.75      0.76      1879\n",
            "    Not Hope       0.77      0.78      0.78      1958\n",
            "\n",
            "    accuracy                           0.77      3837\n",
            "   macro avg       0.77      0.77      0.77      3837\n",
            "weighted avg       0.77      0.77      0.77      3837\n",
            "\n",
            "Classification Report for English (en):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hope       0.77      0.79      0.78       834\n",
            "    Not Hope       0.78      0.75      0.76       816\n",
            "\n",
            "    accuracy                           0.77      1650\n",
            "   macro avg       0.77      0.77      0.77      1650\n",
            "weighted avg       0.77      0.77      0.77      1650\n",
            "\n",
            "Classification Report for Urdu (ur):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hope       0.75      0.93      0.83       794\n",
            "    Not Hope       0.92      0.73      0.81       884\n",
            "\n",
            "    accuracy                           0.82      1678\n",
            "   macro avg       0.83      0.83      0.82      1678\n",
            "weighted avg       0.84      0.82      0.82      1678\n",
            "\n",
            "Classification Report for German (de):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hope       0.80      0.76      0.78      1790\n",
            "    Not Hope       0.83      0.86      0.85      2418\n",
            "\n",
            "    accuracy                           0.82      4208\n",
            "   macro avg       0.82      0.81      0.81      4208\n",
            "weighted avg       0.82      0.82      0.82      4208\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test set**"
      ],
      "metadata": {
        "id": "9R2-Pwaia9Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_es = pipeline.predict(es_test['processed_text'])\n",
        "y_pred_en = pipeline.predict(en_test['processed_text'])\n",
        "y_pred_ur = pipeline.predict(ur_test['processed_text'])\n",
        "y_pred_de = pipeline.predict(de_test['processed_text'])"
      ],
      "metadata": {
        "id": "TactiApkZlO3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#submission file\n",
        "\n",
        "y_pred_es = pd.DataFrame(data=y_pred_es, columns=['final_prediction'])\n",
        "Submisssion_es = pd.DataFrame()\n",
        "Submisssion_es['Text'] = es_test['text']\n",
        "Submisssion_es['Tag'] = y_pred_es\n",
        "Submisssion_es.to_csv('predictions.csv', index = None)"
      ],
      "metadata": {
        "id": "qmCw2OEaZp39"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#submission file\n",
        "\n",
        "y_pred_en = pd.DataFrame(data=y_pred_en, columns=['final_prediction'])\n",
        "Submisssion_en = pd.DataFrame()\n",
        "Submisssion_en['Text'] = en_test['text']\n",
        "Submisssion_en['Tag'] = y_pred_en\n",
        "Submisssion_en.to_csv('predictions.csv', index = None)"
      ],
      "metadata": {
        "id": "VcK9UC8MZseG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#submission file\n",
        "\n",
        "y_pred_ur = pd.DataFrame(data=y_pred_ur, columns=['final_prediction'])\n",
        "Submisssion_ur = pd.DataFrame()\n",
        "Submisssion_ur['Text'] = ur_test['text']\n",
        "Submisssion_ur['Tag'] = y_pred_ur\n",
        "Submisssion_ur.to_csv('predictions.csv', index = None)"
      ],
      "metadata": {
        "id": "t5yCuFfPZu7N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#submission file\n",
        "\n",
        "y_pred_de = pd.DataFrame(data=y_pred_de, columns=['final_prediction'])\n",
        "Submisssion_de = pd.DataFrame()\n",
        "Submisssion_de['Text'] = de_test['text']\n",
        "Submisssion_de['Tag'] = y_pred_de\n",
        "Submisssion_de.to_csv('predictions.csv', index = None)"
      ],
      "metadata": {
        "id": "DuZ2UyO2ZxT1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multiclass**"
      ],
      "metadata": {
        "id": "9-jQpy3tapGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DEV set**"
      ],
      "metadata": {
        "id": "6BT09SyZauuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
        "    ('clf', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "pipeline.fit(merge_train['processed_text'], merge_train['multiclass'])\n",
        "\n",
        "\n",
        "y_pred_es = pipeline.predict(es_dev['processed_text'])\n",
        "y_pred_en = pipeline.predict(en_dev['processed_text'])\n",
        "y_pred_ur = pipeline.predict(ur_dev['processed_text'])\n",
        "y_pred_de = pipeline.predict(de_dev['processed_text'])\n"
      ],
      "metadata": {
        "id": "3wPCvqQHxDj_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Classification Report for Spanish (es):\")\n",
        "print(classification_report(es_dev['multiclass'], y_pred_es))\n",
        "\n",
        "print(\"Classification Report for English (en):\")\n",
        "print(classification_report(en_dev['multiclass'], y_pred_en))\n",
        "\n",
        "print(\"Classification Report for Urdu (ur):\")\n",
        "print(classification_report(ur_dev['multiclass'], y_pred_ur))\n",
        "\n",
        "print(\"Classification Report for German (de):\")\n",
        "print(classification_report(de_dev['multiclass'], y_pred_de))"
      ],
      "metadata": {
        "id": "GG6XZPAExQtx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "151fddc1-7a00-4a92-fd7d-51ee60b135b1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Spanish (es):\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Generalized Hope       0.61      0.62      0.61      1001\n",
            "        Not Hope       0.71      0.87      0.78      1958\n",
            "  Realistic Hope       0.54      0.20      0.29       405\n",
            "Unrealistic Hope       0.52      0.29      0.37       473\n",
            "\n",
            "        accuracy                           0.66      3837\n",
            "       macro avg       0.59      0.49      0.51      3837\n",
            "    weighted avg       0.64      0.66      0.63      3837\n",
            "\n",
            "Classification Report for English (en):\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Generalized Hope       0.55      0.64      0.59       467\n",
            "        Not Hope       0.71      0.85      0.77       816\n",
            "  Realistic Hope       0.56      0.14      0.23       196\n",
            "Unrealistic Hope       0.57      0.27      0.37       171\n",
            "\n",
            "        accuracy                           0.64      1650\n",
            "       macro avg       0.60      0.47      0.49      1650\n",
            "    weighted avg       0.63      0.64      0.61      1650\n",
            "\n",
            "Classification Report for Urdu (ur):\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Generalized Hope       0.40      0.10      0.17       403\n",
            "        Not Hope       0.54      0.96      0.69       884\n",
            "  Realistic Hope       0.00      0.00      0.00       120\n",
            "Unrealistic Hope       0.33      0.01      0.01       271\n",
            "\n",
            "        accuracy                           0.53      1678\n",
            "       macro avg       0.32      0.27      0.22      1678\n",
            "    weighted avg       0.44      0.53      0.41      1678\n",
            "\n",
            "Classification Report for German (de):\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Generalized Hope       0.73      0.71      0.72      1353\n",
            "        Not Hope       0.79      0.91      0.85      2418\n",
            "  Realistic Hope       0.57      0.20      0.30       277\n",
            "Unrealistic Hope       0.46      0.07      0.12       160\n",
            "\n",
            "        accuracy                           0.76      4208\n",
            "       macro avg       0.64      0.47      0.49      4208\n",
            "    weighted avg       0.74      0.76      0.74      4208\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Set**"
      ],
      "metadata": {
        "id": "8pGeAV7eai2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_es = pipeline.predict(es_test['processed_text'])\n",
        "y_pred_en = pipeline.predict(en_test['processed_text'])\n",
        "y_pred_ur = pipeline.predict(ur_test['processed_text'])\n",
        "y_pred_de = pipeline.predict(de_test['processed_text'])"
      ],
      "metadata": {
        "id": "EmrOR6bUZ2Du"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#submission file\n",
        "\n",
        "y_pred_es = pd.DataFrame(data=y_pred_es, columns=['final_prediction'])\n",
        "Submisssion_es = pd.DataFrame()\n",
        "Submisssion_es['Text'] = es_test['text']\n",
        "Submisssion_es['Tag'] = y_pred_es\n",
        "Submisssion_es.to_csv('predictions.csv', index = None)"
      ],
      "metadata": {
        "id": "Fa46v0irYT-H"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#submission file\n",
        "\n",
        "y_pred_en = pd.DataFrame(data=y_pred_en, columns=['final_prediction'])\n",
        "Submisssion_en = pd.DataFrame()\n",
        "Submisssion_en['Text'] = en_test['text']\n",
        "Submisssion_en['Tag'] = y_pred_en\n",
        "Submisssion_en.to_csv('predictions.csv', index = None)"
      ],
      "metadata": {
        "id": "9Q_dUYQVYU--"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "JhvazEf1WWqD"
      },
      "outputs": [],
      "source": [
        "#submission file\n",
        "\n",
        "y_pred_ur = pd.DataFrame(data=y_pred_ur, columns=['final_prediction'])\n",
        "Submisssion_ur = pd.DataFrame()\n",
        "Submisssion_ur['Text'] = ur_test['text']\n",
        "Submisssion_ur['Tag'] = y_pred_ur\n",
        "Submisssion_ur.to_csv('predictions.csv', index = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iia9pH8QWWsI"
      },
      "outputs": [],
      "source": [
        "#submission file\n",
        "\n",
        "y_pred_de = pd.DataFrame(data=y_pred_de, columns=['final_prediction'])\n",
        "Submisssion_de = pd.DataFrame()\n",
        "Submisssion_de['Text'] = de_test['text']\n",
        "Submisssion_de['Tag'] = y_pred_de\n",
        "Submisssion_de.to_csv('predictions.csv', index = None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xEcEx8pLaAEU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9Sg70sU7op921uXIRDAzi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}